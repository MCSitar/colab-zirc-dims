{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnVupBBn9eR"
      },
      "source": [
        "# Grain instance segmentation model training notebook; (extensively) modified from Detectron2 training tutorial\n",
        "\n",
        "This Colab Notebook will allow users to train new models to detect and segment detrital zircon from RL images using Detectron2 and the training dataset provided in the colab_zirc_dims repo. It is set up to train a Mask RCNN model (ResNet depth=101), but could be modified for other instance segmentation models provided that they are supported by Detectron2.\n",
        "\n",
        "The training dataset should be uploaded to the user's Google Drive before running this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII"
      },
      "source": [
        "## Install detectron2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsePPpwZSmqt"
      },
      "outputs": [],
      "source": [
        "!pip install pyyaml==5.1\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "import torch\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "\n",
        "### Standard Detectron2 installation code below; \\\n",
        "### uncomment once D2 binary dist. is released for Torch V1.11.\n",
        "## Install detectron2 that matches the above pytorch version\n",
        "## See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "#!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n",
        "#exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "outputs": [],
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "import copy\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "import random\n",
        "import shutil\n",
        "import torch\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2.engine.hooks import HookBase\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.evaluation import inference_context, COCOEvaluator\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.utils.logger import log_every_n_seconds\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_train_loader, DatasetMapper, build_detection_test_loader\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2.config import LazyConfig\n",
        "import detectron2.data.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpVR_g9pTRXz"
      },
      "outputs": [],
      "source": [
        "#check the GPU on current Colab VM\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1mFydlUY00A"
      },
      "source": [
        "## Mount Google Drive, set path to model saving directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBfk6JlPYypJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90RmxiInbQLc"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Add path to model saving directory (automatically created if it does not yet exist)\n",
        "model_save_dir = '/content/drive/MyDrive/YOUR_MODEL_SAVING_DIRECTORY_HERE' #@param {type:\"string\"}\n",
        "\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "#@markdown ### Add a base name for the model you are training\n",
        "model_save_name = 'YOUR_MODEL_NAME_HERE' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Run this cell after setting the above strings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download, unzip dataset"
      ],
      "metadata": {
        "id": "dhAQA4rfgVmY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziUHBw-Hmre_"
      },
      "outputs": [],
      "source": [
        "#download and unzip 'czd_large' dataset\n",
        "!wget https://czdtrainingdatasetlarge.s3.amazonaws.com/CZD_train_large.zip\n",
        "\n",
        "os.makedirs('/content/training_dataset', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/CZD_train_large.zip -d /content/training_dataset\n",
        "dataset_dir = '/content/training_dataset'"
      ],
      "metadata": {
        "id": "r2xIAyHtnjrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ipSfPy8rNyU"
      },
      "outputs": [],
      "source": [
        "dataset_dir = '/content/training_dataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bksg-AInoIeZ"
      },
      "source": [
        "## Import train, val catalogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9y0C9pUbOHN"
      },
      "outputs": [],
      "source": [
        "from detectron2.structures import BoxMode\n",
        "\n",
        "\n",
        "# a function to convert Via image annotation .json dict format to Detectron2 \\\n",
        "# training input dict format\n",
        "def get_grain_dicts(img_dir):\n",
        "    json_file = os.path.join(img_dir, \"via_region_data.json\")\n",
        "    with open(json_file) as f:\n",
        "        imgs_anns = json.load(f)['_via_img_metadata']\n",
        "\n",
        "    dataset_dicts = []\n",
        "    for idx, v in enumerate(imgs_anns.values()):\n",
        "        record = {}\n",
        "        \n",
        "        filename = os.path.join(img_dir, v[\"filename\"])\n",
        "        height, width = cv2.imread(filename).shape[:2]\n",
        "        \n",
        "        record[\"file_name\"] = filename\n",
        "        record[\"image_id\"] = idx\n",
        "        record[\"height\"] = height\n",
        "        record[\"width\"] = width\n",
        "      \n",
        "        #annos = v[\"regions\"]\n",
        "        annos = {}\n",
        "        for n, eachitem in enumerate(v['regions']):\n",
        "          annos[str(n)] = eachitem\n",
        "        objs = []\n",
        "        for _, anno in annos.items():\n",
        "            #assert not anno[\"region_attributes\"]\n",
        "            anno = anno[\"shape_attributes\"]\n",
        "            px = anno[\"all_points_x\"]\n",
        "            py = anno[\"all_points_y\"]\n",
        "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
        "            poly = [p for x in poly for p in x]\n",
        "\n",
        "            obj = {\n",
        "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"segmentation\": [poly],\n",
        "                \"category_id\": 0,\n",
        "            }\n",
        "            objs.append(obj)\n",
        "        record[\"annotations\"] = objs\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "#registers training, val datasets (converts annotations using get_grain_dicts)\n",
        "for d in [\"train\", \"val\"]:\n",
        "    print('Registering:', d)\n",
        "    DatasetCatalog.register(\"grain_\" + d, lambda d=d: get_grain_dicts(dataset_dir + \"/\" + d))\n",
        "    MetadataCatalog.get(\"grain_\" + d).set(thing_classes=[\"grain\"])\n",
        "grain_metadata = MetadataCatalog.get(\"grain_train\")\n",
        "\n",
        "train_cat = DatasetCatalog.get(\"grain_train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiWYwSS0eHWQ"
      },
      "source": [
        "## Visualize train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkNbUzUOLYf0"
      },
      "outputs": [],
      "source": [
        "# visualize random sample from training dataset\n",
        "dataset_dicts = get_grain_dicts(os.path.join(dataset_dir, 'train'))\n",
        "for d in random.sample(dataset_dicts, 4): #change int here to change sample size\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=grain_metadata, scale=0.5)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1-3zSWRn4H3"
      },
      "source": [
        "## Define Augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxelWqWDY7nZ"
      },
      "source": [
        "The cells below defines augmentations used while training to ensure that models never see the same exact image twice during training. This mitigates overfitting and allows models to achieve substantially higher accuracy in their segmentations/measurements."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define custom tiled defocus-like blur augmentation (mimics artefacts in some mosaic-style LA-ICP-MS images)"
      ],
      "metadata": {
        "id": "yqJRnIBZfFof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The following code is modified from the imagecorruptions package.\n",
        "# Please see the Apache 2.0 license file at:\n",
        "# https://github.com/bethgelab/imagecorruptions\n",
        "\n",
        "def disk(radius, alias_blur=0.1, dtype=np.float32):\n",
        "    if radius <= 8:\n",
        "        L = np.arange(-8, 8 + 1)\n",
        "        ksize = (3, 3)\n",
        "    else:\n",
        "        L = np.arange(-radius, radius + 1)\n",
        "        ksize = (5, 5)\n",
        "    X, Y = np.meshgrid(L, L)\n",
        "    aliased_disk = np.array((X ** 2 + Y ** 2) <= radius ** 2, dtype=dtype)\n",
        "    aliased_disk /= np.sum(aliased_disk)\n",
        "\n",
        "    # supersample disk to antialias\n",
        "    return cv2.GaussianBlur(aliased_disk, ksize=ksize, sigmaX=alias_blur)\n",
        "\n",
        "\n",
        "def defocus_blur(x, severity=1):\n",
        "    c = [(3, 0.1), (4, 0.5), (6, 0.5), (8, 0.5), (10, 0.5), (12, 0.5)][severity - 1]\n",
        "\n",
        "    x = np.array(x) / 255.\n",
        "    kernel = disk(radius=c[0], alias_blur=c[1])\n",
        "\n",
        "    channels = []\n",
        "    if len(x.shape) < 3 or x.shape[2] < 3:\n",
        "        channels = np.array(cv2.filter2D(x, -1, kernel))\n",
        "    else:\n",
        "        for d in range(3):\n",
        "            channels.append(cv2.filter2D(x[:, :, d], -1, kernel))\n",
        "        channels = np.array(channels).transpose((1, 2, 0))\n",
        "\n",
        "    return np.around(np.clip(channels, 0, 1) * 255).astype(int)\n",
        "# END MODIFIED CODE\n",
        "\n",
        "def defocus_random_x(inpt_img, severity=1):\n",
        "    x_loc = random.randint(1, inpt_img.shape[1]-1)\n",
        "    out_img = np.copy(inpt_img)\n",
        "    if bool(random.getrandbits(1)):\n",
        "        out_img[:,x_loc:, :] = defocus_blur(out_img[:,x_loc:, :], severity=severity)\n",
        "    else:\n",
        "        out_img[:,:x_loc, :] = defocus_blur(out_img[:,:x_loc, :], severity=severity)\n",
        "    return out_img\n",
        "\n",
        "def defocus_random_y(inpt_img, severity=1):\n",
        "    y_loc = random.randint(1, inpt_img.shape[0]-1)\n",
        "    out_img = np.copy(inpt_img)\n",
        "    if bool(random.getrandbits(1)):\n",
        "        out_img[y_loc:,:, :] = defocus_blur(out_img[y_loc:,:, :], severity=severity)\n",
        "    else:\n",
        "        out_img[:y_loc,:, :] = defocus_blur(out_img[:y_loc,:, :], severity=severity)\n",
        "    return out_img\n",
        "\n",
        "def defocus_random_patch(inpt_img, severity=1):\n",
        "    x_loc = random.randint(1, inpt_img.shape[1]-1)\n",
        "    y_loc = random.randint(1, inpt_img.shape[0]-1)\n",
        "    out_img = np.copy(inpt_img)\n",
        "    if bool(random.getrandbits(1)):\n",
        "        if bool(random.getrandbits(1)):\n",
        "            out_img[y_loc:,x_loc:, :] = defocus_blur(out_img[y_loc:,x_loc:,:], severity=severity)\n",
        "        else:\n",
        "            out_img[y_loc:,:x_loc, :] = defocus_blur(out_img[y_loc:,:x_loc,:], severity=severity)\n",
        "    else:\n",
        "        if bool(random.getrandbits(1)):\n",
        "            out_img[:y_loc,x_loc:, :] = defocus_blur(out_img[:y_loc,x_loc:, :], severity=severity)\n",
        "        else:\n",
        "            out_img[:y_loc,:x_loc, :] = defocus_blur(out_img[:y_loc,:x_loc, :], severity=severity)\n",
        "    return out_img\n",
        "\n",
        "def random_defocus(inpt_img):\n",
        "    fxn = random.choice([defocus_random_x, \n",
        "                         defocus_random_y,\n",
        "                         defocus_random_patch])\n",
        "    severity=random.choice([2, 3, 4, 5, 6])\n",
        "    return fxn(inpt_img, severity=severity)\n",
        "\n",
        "\n",
        "#modified from https://gist.github.com/mallyagirish/1885443642b3b7bf438a20f216df0dc3\n",
        "\n",
        "class TiledBlurTransform(T.Transform):\n",
        "    \"\"\"\n",
        "    Transform a random part of an image using defocus blur.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._set_attributes(locals())\n",
        "\n",
        "    def apply_image(self, img: np.ndarray, interp: str = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Apply random defocus blur transform on the image(s).\n",
        "        Args:\n",
        "            img (ndarray): of shape NxHxWxC, or HxWxC or HxW. The array can be\n",
        "                of type uint8 in range [0, 255], or floating point in range\n",
        "                [0, 1] or [0, 255].\n",
        "            interp (str): keep this option for consistency, gaussian blur would not\n",
        "                require interpolation.\n",
        "        Returns:\n",
        "            ndarray: blurred image(s).\n",
        "        \"\"\"\n",
        "        if img.ndim == 2:\n",
        "            img[:, :] = defocus_blur(img[:, :], severity=4) #not expected; just apply a standard blur if found\n",
        "        elif img.ndim == 3:\n",
        "            img = random_defocus(img)\n",
        "        elif img.ndim == 4:\n",
        "            for each_n in img.shape[0]:\n",
        "                img[each_n, :, :, :] = random_defocus(img[each_n, :, :, :])\n",
        "        return img\n",
        "\n",
        "    def apply_coords(self, coords: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Apply no transform on the coordinates.\n",
        "        \"\"\"\n",
        "        return coords\n",
        "\n",
        "    def apply_segmentation(self, segmentation: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Apply no transform on the full-image segmentation.\n",
        "        \"\"\"\n",
        "        return segmentation\n",
        "\n",
        "    def inverse(self) -> T.Transform:\n",
        "        \"\"\"\n",
        "        The inverse is a no-op.\n",
        "        \"\"\"\n",
        "        return T.NoOpTransform()\n",
        "# END MODIFIED CODE\n",
        "\t\t\n",
        "class RandomTiledDefocusBlur(T.Augmentation):\n",
        "    \"\"\"\n",
        "    Apply Random defocus blur on patch or section of image.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self._init(locals())\n",
        "\n",
        "    def get_transform(self, image):\n",
        "        return TiledBlurTransform()"
      ],
      "metadata": {
        "id": "OmjEuYW0fFJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define custom transform list"
      ],
      "metadata": {
        "id": "LrlhRyKHfher"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VU2nQL3n63D"
      },
      "outputs": [],
      "source": [
        "custom_transform_list = [\n",
        "                         T.RandomApply(RandomTiledDefocusBlur(), prob=.05), #randomly apply our custom 'tiled blur' augmentation at low probabability\n",
        "                         T.RandomCrop('relative', (0.95, 0.95)), #randomly crop an area (95% size of original) from image\n",
        "                         T.RandomLighting(100), #minor lighting randomization\n",
        "                         T.RandomContrast(.85, 1.15), #minor contrast randomization\n",
        "                         T.RandomFlip(prob=.5, horizontal=False, vertical=True), #random vertical flipping\n",
        "                         T.RandomFlip(prob=.5, horizontal=True, vertical=False),  #and horizontal flipping\n",
        "                         T.RandomApply(T.RandomRotation([-30, 30], False), prob=.8), #random (80% probability) rotation up to 30 degrees; \\\n",
        "                                                                                     # more rotation does not seem to improve results\n",
        "                         T.ResizeShortestEdge([400,800])\n",
        "                         ] # resize img again for multiscale training. Set to ([800, 800]) for single-\\\n",
        "                                                          # scale training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize transforms"
      ],
      "metadata": {
        "id": "eQ-gxo1fjgur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#optionally, visualize your augmentation when applied to an image from the dataset\n",
        "test_augs = T.AugmentationList(custom_transform_list)  # type: T.Augmentation\n",
        "\n",
        "# visualize random sample from training dataset\n",
        "dataset_dicts = get_grain_dicts(os.path.join(dataset_dir, 'train'))\n",
        "test_img_dict = dataset_dicts[20] #change idx here to change the sample shown\n",
        "test_img = cv2.imread(test_img_dict['file_name'])\n",
        "cv2_imshow(test_img)\n",
        "for i in range(10): #change int here to increase number of tests\n",
        "  input = T.AugInput(image = test_img)\n",
        "  transform = test_augs(input)  # type: T.Transform\n",
        "  image_transformed = input.image\n",
        "\n",
        "  cv2_imshow(image_transformed)"
      ],
      "metadata": {
        "id": "KiKnMg45jc_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bjrfb2LDeo"
      },
      "source": [
        "## Define dataset mapper, training, checkpointing loss eval functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIbAM2pv-urF"
      },
      "outputs": [],
      "source": [
        "from detectron2.engine import DefaultTrainer, AMPTrainer, SimpleTrainer, TrainerBase, create_ddp_model\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from detectron2.checkpoint.c2_model_loading import align_and_update_state_dicts\n",
        "from detectron2.data import DatasetMapper\n",
        "from detectron2.utils.file_io import PathManager\n",
        "\n",
        "from fvcore.common.checkpoint import Checkpointer\n",
        "from typing import Any, Dict, Iterable, List, NamedTuple, Optional, Tuple, Set\n",
        "import torch.nn as nn\n",
        "\n",
        "from detectron2.config import CfgNode\n",
        "from detectron2.solver.build import get_default_optimizer_params, maybe_add_gradient_clipping\n",
        "\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "import math\n",
        "import operator\n",
        "import weakref\n",
        "import pickle\n",
        "\n",
        "# loss eval hook for getting vaidation loss, copying to metrics.json; \\\n",
        "# from https://gist.github.com/ortegatron/c0dad15e49c2b74de8bb09a5615d9f6b\n",
        "class LossEvalHook(HookBase):\n",
        "    def __init__(self, eval_period, model, data_loader):\n",
        "        self._model = model\n",
        "        self._period = eval_period\n",
        "        self._data_loader = data_loader\n",
        "    \n",
        "    def _do_loss_eval(self):\n",
        "        # Copying inference_on_dataset from evaluator.py\n",
        "        total = len(self._data_loader)\n",
        "        num_warmup = min(5, total - 1)\n",
        "            \n",
        "        start_time = time.perf_counter()\n",
        "        total_compute_time = 0\n",
        "        losses = []\n",
        "        for idx, inputs in enumerate(self._data_loader):            \n",
        "            if idx == num_warmup:\n",
        "                start_time = time.perf_counter()\n",
        "                total_compute_time = 0\n",
        "            start_compute_time = time.perf_counter()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            total_compute_time += time.perf_counter() - start_compute_time\n",
        "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
        "            seconds_per_img = total_compute_time / iters_after_start\n",
        "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
        "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
        "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
        "                log_every_n_seconds(\n",
        "                    logging.INFO,\n",
        "                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
        "                        idx + 1, total, seconds_per_img, str(eta)\n",
        "                    ),\n",
        "                    n=5,\n",
        "                )\n",
        "            loss_batch = self._get_loss(inputs)\n",
        "            losses.append(loss_batch)\n",
        "        mean_loss = np.mean(losses)\n",
        "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
        "        #self.trainer.storage.put_scalar('')\n",
        "        comm.synchronize()\n",
        "\n",
        "        return losses\n",
        "            \n",
        "    def _get_loss(self, data):\n",
        "        # How loss is calculated on train_loop \n",
        "        metrics_dict = self._model(data)\n",
        "        metrics_dict = {\n",
        "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
        "            for k, v in metrics_dict.items()\n",
        "        }\n",
        "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
        "        return total_losses_reduced\n",
        "        \n",
        "        \n",
        "    def after_step(self):\n",
        "        next_iter = self.trainer.iter + 1\n",
        "        is_final = next_iter == self.trainer.max_iter\n",
        "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
        "            self._do_loss_eval()\n",
        "\n",
        "#Save each checkpoint if validation loss is at a minima\n",
        "# modified from \n",
        "class BestCheckpointer(HookBase):\n",
        "    \"\"\"\n",
        "    Checkpoints best weights based off given metric.\n",
        "    This hook should be used in conjunction to and executed after the hook\n",
        "    that produces the metric, e.g. `EvalHook`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        eval_period: int,\n",
        "        inpt_checkpointer: Checkpointer,\n",
        "        val_metric: str,\n",
        "        custom_save_dir: str,\n",
        "        model,\n",
        "        mode: str = \"min\",\n",
        "        file_prefix: str = \"model_best\",\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            eval_period (int): the period `EvalHook` is set to run.\n",
        "            checkpointer: the checkpointer object used to save checkpoints.\n",
        "            val_metric (str): validation metric to track for best checkpoint, e.g. \"bbox/AP50\"\n",
        "            mode (str): one of {'max', 'min'}. controls whether the chosen val metric should be\n",
        "                maximized or minimized, e.g. for \"bbox/AP50\" it should be \"max\"\n",
        "            file_prefix (str): the prefix of checkpoint's filename, defaults to \"model_best\"\n",
        "        \"\"\"\n",
        "        #self._logger = logging.getLogger(__name__)\n",
        "        self._period = eval_period\n",
        "        self._val_metric = val_metric\n",
        "        assert mode in [\n",
        "            \"max\",\n",
        "            \"min\",\n",
        "        ], f'Mode \"{mode}\" to `BestCheckpointer` is unknown. It should be one of {\"max\", \"min\"}.'\n",
        "        if mode == \"max\":\n",
        "            self._compare = operator.gt\n",
        "        else:\n",
        "            self._compare = operator.lt\n",
        "        self._checkpointer = inpt_checkpointer\n",
        "        self._file_prefix = file_prefix\n",
        "        self.custom_save_dir = custom_save_dir\n",
        "        self.best_metric = None\n",
        "        self.best_iter = None\n",
        "        self._model = model\n",
        "\n",
        "    def _update_best(self, val, iteration):\n",
        "        if math.isnan(val) or math.isinf(val):\n",
        "            return False\n",
        "        self.best_metric = val\n",
        "        self.best_iter = iteration\n",
        "        return True\n",
        "\n",
        "    def _best_checking(self):\n",
        "        metric_tuple = self.trainer.storage.latest().get(self._val_metric)\n",
        "        if metric_tuple is None:\n",
        "            return\n",
        "            #self._logger.warning(\n",
        "            #    f\"Given val metric {self._val_metric} does not seem to be computed/stored.\"\n",
        "            #    \"Will not be checkpointing based on it.\"\n",
        "            #)\n",
        "           # return\n",
        "        else:\n",
        "            latest_metric, metric_iter = metric_tuple\n",
        "\n",
        "        if self.best_metric is None:\n",
        "            if self._update_best(latest_metric, metric_iter):\n",
        "                additional_state = {\"iteration\": metric_iter}\n",
        "                iter_str = str(round(self.best_iter/1000, 1)) + 'k'\n",
        "                full_model_save_pth = os.path.join(self.custom_save_dir,\n",
        "                                                   f\"{self._file_prefix}\"+'_'+iter_str+'.pth')\n",
        "                torch.save(self._model.state_dict(), full_model_save_pth)\n",
        "                #old_save_dir = copy.deepcopy(self._checkpointer.save_dir)\n",
        "                #self._checkpointer.save_dir = self.custom_save_dir\n",
        "                #self._checkpointer.save(f\"{self._file_prefix}\"+'_'+iter_str, **additional_state)\n",
        "                #self._checkpointer.save_dir = old_save_dir\n",
        "                #self._logger.info(\n",
        "                #    f\"Saved first model at {self.best_metric:0.5f} @ {self.best_iter} steps\"\n",
        "                #)\n",
        "        elif self._compare(latest_metric, self.best_metric):\n",
        "            additional_state = {\"iteration\": metric_iter}\n",
        "            iter_str = str(round(metric_iter/1000, 1)) + 'k'\n",
        "            old_iter_str = str(round(self.best_iter/1000, 1)) + 'k'\n",
        "            old_iter_path = os.path.join(self.custom_save_dir, \n",
        "                                         f\"{self._file_prefix}\"+'_'+old_iter_str+'.pth')\n",
        "            if os.path.isfile(old_iter_path):\n",
        "                os.remove(old_iter_path)\n",
        "            full_model_save_pth = os.path.join(self.custom_save_dir,\n",
        "                                                f\"{self._file_prefix}\"+'_'+iter_str+'.pth')\n",
        "            torch.save(self._model.state_dict(), full_model_save_pth)\n",
        "            #print(self._checkpointer.custom_save_dir)\n",
        "            #old_save_dir = copy.deepcopy(self._checkpointer.save_dir)\n",
        "            #self._checkpointer.save_dir = self.custom_save_dir\n",
        "            #self._checkpointer.save(f\"{self._file_prefix}\"+'_'+iter_str, **additional_state)\n",
        "            #self._checkpointer.save_dir = old_save_dir\n",
        "            #self._logger.info(\n",
        "            #    f\"Saved best model as latest eval score for {self._val_metric} is \"\n",
        "            #    f\"{latest_metric:0.5f}, better than last best score \"\n",
        "            #    f\"{self.best_metric:0.5f} @ iteration {self.best_iter}.\"\n",
        "            #)\n",
        "            self._update_best(latest_metric, metric_iter)\n",
        "        else:\n",
        "            pass\n",
        "            #self._logger.info(\n",
        "            #    f\"Not saving as latest eval score for {self._val_metric} is {latest_metric:0.5f}, \"\n",
        "            #    f\"not better than best score {self.best_metric:0.5f} @ iteration {self.best_iter}.\"\n",
        "            #)\n",
        "\n",
        "    def after_step(self):\n",
        "        # same conditions as `EvalHook`\n",
        "        next_iter = self.trainer.iter + 1\n",
        "        if (\n",
        "            self._period > 0\n",
        "            and next_iter % self._period == 0\n",
        "            and next_iter != self.trainer.max_iter\n",
        "        ):\n",
        "            self._best_checking()\n",
        "\n",
        "    def after_train(self):\n",
        "        # same conditions as `EvalHook`\n",
        "        if self.trainer.iter + 1 >= self.trainer.max_iter:\n",
        "            self._best_checking()\n",
        "\n",
        "\n",
        "# loss eval hook for getting vaidation loss, copying to metrics.json; \\\n",
        "# from https://gist.github.com/ortegatron/c0dad15e49c2b74de8bb09a5615d9f6b\n",
        "class BackupToDriveHook(HookBase):\n",
        "    def __init__(self, checkpoint_period, model_name, model_save_dir, inpt_checkpointer,\n",
        "                 curr_metrics_dir, model):\n",
        "        self._period = checkpoint_period\n",
        "        self.model_name = str(model_name)\n",
        "        self.model_save_dir = model_save_dir\n",
        "        self._checkpointer = inpt_checkpointer\n",
        "        self.curr_metrics_dir = curr_metrics_dir\n",
        "        self._model = model\n",
        "\n",
        "      \n",
        "    \n",
        "    def _backup_model_and_metrics(self):\n",
        "\n",
        "        root_output_dir = os.path.join(self.model_save_dir, self.model_name) #output_dir = save dir from user input\n",
        "\n",
        "        #creates individual model output directory if it does not already exist\n",
        "        os.makedirs(root_output_dir, exist_ok=True)\n",
        "        #creates a name for this version of model; include iteration number\n",
        "        curr_iters_str = str(round((self.trainer.iter + 1)/1000, 1)) + 'k'\n",
        "        curr_model_name = self.model_name + '_' + curr_iters_str\n",
        "        #model_save_pth = os.path.join(root_output_dir, curr_model_name)\n",
        "\n",
        "        #get most recent model, current metrics, copy to drive\n",
        "        metrics_path = os.path.join(self.curr_metrics_dir, 'metrics.json')\n",
        "\n",
        "        shutil.copy(metrics_path, self.model_save_dir)\n",
        "        full_model_save_pth = os.path.join(self.model_save_dir,\n",
        "                                            curr_model_name+'.pth')\n",
        "        torch.save(self._model.state_dict(), full_model_save_pth)\n",
        "        #old_save_dir = copy.deepcopy(self._checkpointer.save_dir)\n",
        "        #self._checkpointer.save_dir = self.model_save_dir\n",
        "        #self._checkpointer.save(curr_model_name)\n",
        "        #self._checkpointer.save_dir = old_save_dir\n",
        "\n",
        "        comm.synchronize()\n",
        "        \n",
        "        \n",
        "    def after_step(self):\n",
        "        next_iter = self.trainer.iter + 1\n",
        "        is_final = next_iter == self.trainer.max_iter\n",
        "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
        "            self._backup_model_and_metrics()\n",
        "\n",
        "#trainer for grains which incorporates augmentation, hooks for eval\n",
        "class GrainTrainer(DefaultTrainer):\n",
        "    \n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        #return a custom train loader with augmentations; recompute_boxes \\\n",
        "        # is important given cropping, rotation augs\n",
        "        return build_detection_train_loader(cfg, mapper=\n",
        "                                            DatasetMapper(cfg, is_train=True, recompute_boxes = True,\n",
        "                                                          augmentations = custom_transform_list\n",
        "                                                          ),\n",
        "                                            )\n",
        "    #use Adam optimizer, which seems to work better(?) than stochastic gradient\\\n",
        "    # descent (SGD) when training with czd_large\n",
        "    @classmethod\n",
        "    def build_optimizer(cls, cfg, model):\n",
        "        \"\"\"\n",
        "        Build an optimizer from config.\n",
        "        \"\"\"\n",
        "        norm_module_types = (\n",
        "            torch.nn.BatchNorm1d,\n",
        "            torch.nn.BatchNorm2d,\n",
        "            torch.nn.BatchNorm3d,\n",
        "            torch.nn.SyncBatchNorm,\n",
        "            # NaiveSyncBatchNorm inherits from BatchNorm2d\n",
        "            torch.nn.GroupNorm,\n",
        "            torch.nn.InstanceNorm1d,\n",
        "            torch.nn.InstanceNorm2d,\n",
        "            torch.nn.InstanceNorm3d,\n",
        "            torch.nn.LayerNorm,\n",
        "            torch.nn.LocalResponseNorm,\n",
        "        )\n",
        "        params: List[Dict[str, Any]] = []\n",
        "        memo: Set[torch.nn.parameter.Parameter] = set()\n",
        "        for module in model.modules():\n",
        "            for key, value in module.named_parameters(recurse=False):\n",
        "                if not value.requires_grad:\n",
        "                    continue\n",
        "                # Avoid duplicating parameters\n",
        "                if value in memo:\n",
        "                    continue\n",
        "                memo.add(value)\n",
        "                lr = cfg.SOLVER.BASE_LR\n",
        "                weight_decay = cfg.SOLVER.WEIGHT_DECAY\n",
        "                if isinstance(module, norm_module_types):\n",
        "                    weight_decay = cfg.SOLVER.WEIGHT_DECAY_NORM\n",
        "                elif key == \"bias\":\n",
        "                    # NOTE: unlike Detectron v1, we now default BIAS_LR_FACTOR to 1.0\n",
        "                    # and WEIGHT_DECAY_BIAS to WEIGHT_DECAY so that bias optimizer\n",
        "                    # hyperparameters are by default exactly the same as for regular\n",
        "                    # weights.\n",
        "                    lr = cfg.SOLVER.BASE_LR * cfg.SOLVER.BIAS_LR_FACTOR\n",
        "                    weight_decay = cfg.SOLVER.WEIGHT_DECAY_BIAS\n",
        "                if weight_decay is None:\n",
        "                    weight_decay = 0\n",
        "                params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay}]\n",
        "\n",
        "        optimizer = torch.optim.Adam(params, cfg.SOLVER.BASE_LR)#, momentum=cfg.SOLVER.MOMENTUM) #add momentum for SGD\n",
        "        optimizer = maybe_add_gradient_clipping(cfg, optimizer)\n",
        "        return optimizer\n",
        "  \n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "        if output_folder is None:\n",
        "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
        "    \n",
        "    #set up validation loss eval hook\n",
        "    def build_hooks(self):\n",
        "        hooks = super().build_hooks()\n",
        "        #if comm.is_main_process():\n",
        "        #    hooks.pop(-1)\n",
        "        \n",
        "        hooks.insert(-1, LossEvalHook(\n",
        "            cfg.TEST.EVAL_PERIOD,\n",
        "            self.model,\n",
        "            build_detection_test_loader(\n",
        "                self.cfg,\n",
        "                self.cfg.DATASETS.TEST[0],\n",
        "                DatasetMapper(self.cfg,True)\n",
        "            )\n",
        "        ))\n",
        "        if comm.is_main_process():\n",
        "          hooks.append(BestCheckpointer(self.cfg.TEST.EVAL_PERIOD,\n",
        "                                        self.checkpointer,\n",
        "                                        val_metric='validation_loss',\n",
        "                                        custom_save_dir=model_save_dir,\n",
        "                                        model=self.model,\n",
        "                                        file_prefix=model_save_name+'_best'))\n",
        "          hooks.append(BackupToDriveHook(self.cfg.SOLVER.CHECKPOINT_PERIOD, model_save_name, \n",
        "                                         model_save_dir, self.checkpointer,\n",
        "                                         self.cfg.OUTPUT_DIR,\n",
        "                                         self.model))\n",
        "\n",
        "        return hooks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlqXIXXhW8dA"
      },
      "source": [
        "## Build, train model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqUwuQxhn7aI"
      },
      "source": [
        "\n",
        "### Set max iterations for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXvCcpkfi4yq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#@markdown ### Final iteration before training stops\n",
        "final_iteration = 13000 #@param {type:\"slider\", min:3000, max:15000, step:1000}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VCdB008qoJv"
      },
      "source": [
        "### Actually build and train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb2K56gpnS7m"
      },
      "outputs": [],
      "source": [
        "from detectron2.config import LazyConfig\n",
        "\n",
        "#train from a pre-trained Mask RCNN model\n",
        "cfg = get_cfg()\n",
        "\n",
        "# train from base model: Default Mask RCNN\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))\n",
        "# Load starting weights (COCO trained) from Detectron2 model zoo.\n",
        "cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x/138205316/model_final_a3ec72.pkl\"\n",
        "\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"grain_train\",) #load training dataset\n",
        "cfg.DATASETS.TEST = (\"grain_val\",) # load validation dataset\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2 #2 ims per batch seems to be good for model generalization\n",
        "\n",
        "#use this learning rate for SGD optimizer\n",
        "#cfg.SOLVER.BASE_LR = 0.00025\n",
        "#div LR by 10 for Adam optimizer\n",
        "cfg.SOLVER.BASE_LR = 0.000015 \n",
        "\n",
        "cfg.SOLVER.MAX_ITER = final_iteration\n",
        "\n",
        "#save a checkpoint every 1000 iterations, regardless of validation loss\n",
        "cfg.SOLVER.CHECKPOINT_PERIOD = 1000\n",
        "\n",
        "cfg.SOLVER.GAMMA =  0.5\n",
        "#decay learning rate by factor of GAMMA every 1500 iterations (every ~2 epochs)\n",
        "#cfg.SOLVER.STEPS = (3999, 8000)#SGD steps\n",
        "cfg.SOLVER.STEPS = (1500, 3500, 5000, 6500, 8000, 9000, 10000, 11000) #Adam steps\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # use default ROI heads batch size\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only class here is 'grain'\n",
        "\n",
        "cfg.MODEL.RPN.NMS_THRESH = 0.2 #sets NMS threshold lower than default; should(?) eliminate overlapping regions\n",
        "cfg.TEST.EVAL_PERIOD = 200 # validation eval every 200 iterations\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "#save our config file\n",
        "save_cfg_path=os.path.join(model_save_dir, model_save_name+'.yaml')\n",
        "with open(save_cfg_path, 'w') as f:\n",
        "  cfg.dump(stream=f)\n",
        "\n",
        "\n",
        "trainer = GrainTrainer(cfg) #our grain trainer, w/ built-in augs and val loss eval\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train() #start training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBXeH8UXFcqU"
      },
      "outputs": [],
      "source": [
        "# open tensorboard training metrics curves (metrics.json):\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e4vdDIOXyxF"
      },
      "source": [
        "## Inference & evaluation with final trained model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3CSgYyhtMEV"
      },
      "source": [
        "Initialize model from saved weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya5nEuMELeq8"
      },
      "outputs": [],
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # final model; modify path to other non-final model to view their segmentations\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8  # set a custom testing threshold\n",
        "cfg.MODEL.RPN.NMS_THRESH = 0.2\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWq1XHfDWiXO"
      },
      "source": [
        "View model segmentations for random sample of images from zircon validation dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U5LhISJqWXgM"
      },
      "outputs": [],
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "dataset_dicts = get_grain_dicts(os.path.join(dataset_dir, 'val'))\n",
        "for d in random.sample(dataset_dicts, 100):    \n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=grain_metadata, \n",
        "                   scale=1.5, \n",
        "                   #instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
        "    )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kblA1IyFvWbT"
      },
      "source": [
        "Validation eval with COCO API metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9tECBQCvMv3"
      },
      "outputs": [],
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"grain_val\", (\"bbox\", \"segm\"), False, output_dir=\"./output/\")\n",
        "val_loader = build_detection_test_loader(cfg, \"grain_val\")\n",
        "print(inference_on_dataset(trainer.model, val_loader, evaluator))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ekgkBNOuCYs"
      },
      "source": [
        "## Final notes:\n",
        "\n",
        "To use newly-trained models in colab_zirc_dims:\n",
        "\n",
        "#### Option A:\n",
        "Modify the cell that initializes model(s) in colab_zirc_dims processing notebooks:\n",
        "```\n",
        "predictor = non_std_cfgs.smart_load_predictor(\n",
        "  'PATH_TO_YOUR_CONFIG_YAML_FILE.yaml',\n",
        "  'PATH_TO_YOUR_MODEL_WEIGHTS.pth',\n",
        "  ...\n",
        ")\n",
        "```\n",
        "\n",
        "#### Option B (more complicated but potentially useful for many models):\n",
        "The dynamic model selection tool in colab_zirc_dims is populated from a .json file model library dictionary, which is by default [the current version on the GitHub repo.](https://github.com/MCSitar/colab_zirc_dims/blob/main/czd_model_library.json) The 'url' key in the dict will work with either an AWS download link for the model or the path to model in your Google Drive.\n",
        "\n",
        "To use a custom model library dictionary:\n",
        "Modify a copy of the colab_zirc_dims [.json file model library dictionary](https://github.com/MCSitar/colab_zirc_dims/blob/main/czd_model_library.json) to include download link(s)/Drive path(s) and metadata (e.g., resnet depth and config file) for your model(s). Upload this .json file to your Google Drive and change the 'model_lib_loc' variable in a processing Notebook to the .json's path for dynamic download and loading of this and other models within the Notebook."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}