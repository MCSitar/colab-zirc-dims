{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_zirc_dims_train_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnVupBBn9eR"
      },
      "source": [
        "# Zircon model training notebook; (extensively) modified from Detectron2 training tutorial\n",
        "\n",
        "This Colab Notebook will allow users to train new models to detect and segment detrital zircon from RL images using Detectron2 and the training dataset provided in the colab_zirc_dims repo. It is set up to train a Mask RCNN model (ResNet depth=101), but could be modified for other instance segmentation models provided that they are supported by Detectron2.\n",
        "\n",
        "The training dataset should be uploaded to the user's Google Drive before running this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII"
      },
      "source": [
        "## Install detectron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsePPpwZSmqt"
      },
      "source": [
        "!pip install pyyaml==5.1\n",
        "\n",
        "import torch\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "# Install detectron2 that matches the above pytorch version\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n",
        "exit(0)  # Automatically restarts runtime after installation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "import copy\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "import random\n",
        "import shutil\n",
        "import torch\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2.engine.hooks import HookBase\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.evaluation import inference_context, COCOEvaluator\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.utils.logger import log_every_n_seconds\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_train_loader, DatasetMapper, build_detection_test_loader\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2.config import LazyConfig\n",
        "import detectron2.data.transforms as T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1-3zSWRn4H3"
      },
      "source": [
        "## Define Augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below defines augmentations used while training to ensure that models never see the same exact image twice during training. This mitigates overfitting and allows models to achieve substantially higher accuracy in their segmentations/measurements."
      ],
      "metadata": {
        "id": "VxelWqWDY7nZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VU2nQL3n63D"
      },
      "source": [
        "custom_transform_list = [T.ResizeShortestEdge([800,800]), #resize shortest edge of image to 800 pixels\n",
        "                         T.RandomCrop('relative', (0.95, 0.95)), #randomly crop an area (95% size of original) from image\n",
        "                         T.RandomLighting(100), #minor lighting randomization\n",
        "                         T.RandomContrast(.85, 1.15), #minor contrast randomization\n",
        "                         T.RandomFlip(prob=.5, horizontal=False, vertical=True), #random vertical flipping\n",
        "                         T.RandomFlip(prob=.5, horizontal=True, vertical=False),  #and horizontal flipping\n",
        "                         T.RandomApply(T.RandomRotation([-30, 30], False), prob=.8), #random (80% probability) rotation up to 30 degrees; \\\n",
        "                                                                                     # more rotation does not seem to improve results\n",
        "                         T.ResizeShortestEdge([800,800])] # resize img again for uniformity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive, set paths to dataset, model saving directories"
      ],
      "metadata": {
        "id": "b1mFydlUY00A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fBfk6JlPYypJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Add path to training dataset directory\n",
        "dataset_dir = '/content/drive/MyDrive/training_dataset' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Add path to model saving directory (automatically created if it does not yet exist)\n",
        "model_save_dir = '/content/drive/MyDrive/NAME FOR MODEL SAVING FOLDER HERE' #@param {type:\"string\"}\n",
        "\n",
        "os.makedirs(model_save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "90RmxiInbQLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bjrfb2LDeo"
      },
      "source": [
        "## Define dataset mapper, training, loss eval functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIbAM2pv-urF"
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import DatasetMapper\n",
        "from detectron2.structures import BoxMode\n",
        "\n",
        "# a function to convert Via image annotation .json dict format to Detectron2 \\\n",
        "# training input dict format\n",
        "def get_zircon_dicts(img_dir):\n",
        "    json_file = os.path.join(img_dir, \"via_region_data.json\")\n",
        "    with open(json_file) as f:\n",
        "        imgs_anns = json.load(f)['_via_img_metadata']\n",
        "\n",
        "    dataset_dicts = []\n",
        "    for idx, v in enumerate(imgs_anns.values()):\n",
        "        record = {}\n",
        "        \n",
        "        filename = os.path.join(img_dir, v[\"filename\"])\n",
        "        height, width = cv2.imread(filename).shape[:2]\n",
        "        \n",
        "        record[\"file_name\"] = filename\n",
        "        record[\"image_id\"] = idx\n",
        "        record[\"height\"] = height\n",
        "        record[\"width\"] = width\n",
        "      \n",
        "        #annos = v[\"regions\"]\n",
        "        annos = {}\n",
        "        for n, eachitem in enumerate(v['regions']):\n",
        "          annos[str(n)] = eachitem\n",
        "        objs = []\n",
        "        for _, anno in annos.items():\n",
        "            #assert not anno[\"region_attributes\"]\n",
        "            anno = anno[\"shape_attributes\"]\n",
        "            px = anno[\"all_points_x\"]\n",
        "            py = anno[\"all_points_y\"]\n",
        "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
        "            poly = [p for x in poly for p in x]\n",
        "\n",
        "            obj = {\n",
        "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"segmentation\": [poly],\n",
        "                \"category_id\": 0,\n",
        "            }\n",
        "            objs.append(obj)\n",
        "        record[\"annotations\"] = objs\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "# loss eval hook for getting vaidation loss, copying to metrics.json; \\\n",
        "# from https://gist.github.com/ortegatron/c0dad15e49c2b74de8bb09a5615d9f6b\n",
        "class LossEvalHook(HookBase):\n",
        "    def __init__(self, eval_period, model, data_loader):\n",
        "        self._model = model\n",
        "        self._period = eval_period\n",
        "        self._data_loader = data_loader\n",
        "    \n",
        "    def _do_loss_eval(self):\n",
        "        # Copying inference_on_dataset from evaluator.py\n",
        "        total = len(self._data_loader)\n",
        "        num_warmup = min(5, total - 1)\n",
        "            \n",
        "        start_time = time.perf_counter()\n",
        "        total_compute_time = 0\n",
        "        losses = []\n",
        "        for idx, inputs in enumerate(self._data_loader):            \n",
        "            if idx == num_warmup:\n",
        "                start_time = time.perf_counter()\n",
        "                total_compute_time = 0\n",
        "            start_compute_time = time.perf_counter()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            total_compute_time += time.perf_counter() - start_compute_time\n",
        "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
        "            seconds_per_img = total_compute_time / iters_after_start\n",
        "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
        "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
        "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
        "                log_every_n_seconds(\n",
        "                    logging.INFO,\n",
        "                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
        "                        idx + 1, total, seconds_per_img, str(eta)\n",
        "                    ),\n",
        "                    n=5,\n",
        "                )\n",
        "            loss_batch = self._get_loss(inputs)\n",
        "            losses.append(loss_batch)\n",
        "        mean_loss = np.mean(losses)\n",
        "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
        "        comm.synchronize()\n",
        "\n",
        "        return losses\n",
        "            \n",
        "    def _get_loss(self, data):\n",
        "        # How loss is calculated on train_loop \n",
        "        metrics_dict = self._model(data)\n",
        "        metrics_dict = {\n",
        "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
        "            for k, v in metrics_dict.items()\n",
        "        }\n",
        "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
        "        return total_losses_reduced\n",
        "        \n",
        "        \n",
        "    def after_step(self):\n",
        "        next_iter = self.trainer.iter + 1\n",
        "        is_final = next_iter == self.trainer.max_iter\n",
        "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
        "            self._do_loss_eval()\n",
        "\n",
        "#trainer for zircons which incorporates augmentation, hooks for eval\n",
        "class ZirconTrainer(DefaultTrainer):\n",
        "    \n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        #return a custom train loader with augmentations; recompute_boxes \\\n",
        "        # is important given cropping, rotation augs\n",
        "        return build_detection_train_loader(cfg, mapper=\n",
        "                                            DatasetMapper(cfg, is_train=True, recompute_boxes = True,\n",
        "                                                          augmentations = custom_transform_list\n",
        "                                                          ),\n",
        "                                            )\n",
        "\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "        if output_folder is None:\n",
        "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
        "    \n",
        "    #set up validation loss eval hook\n",
        "    def build_hooks(self):\n",
        "        hooks = super().build_hooks()\n",
        "        hooks.insert(-1,LossEvalHook(\n",
        "            cfg.TEST.EVAL_PERIOD,\n",
        "            self.model,\n",
        "            build_detection_test_loader(\n",
        "                self.cfg,\n",
        "                self.cfg.DATASETS.TEST[0],\n",
        "                DatasetMapper(self.cfg,True)\n",
        "            )\n",
        "        ))\n",
        "        return hooks\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bksg-AInoIeZ"
      },
      "source": [
        "## Import train, val catalogs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9y0C9pUbOHN"
      },
      "source": [
        "#registers training, val datasets (converts annotations using get_zircon_dicts)\n",
        "for d in [\"train\", \"val\"]:\n",
        "    DatasetCatalog.register(\"zircon_\" + d, lambda d=d: get_zircon_dicts(dataset_dir + \"/\" + d))\n",
        "    MetadataCatalog.get(\"zircon_\" + d).set(thing_classes=[\"zircon\"])\n",
        "zircon_metadata = MetadataCatalog.get(\"zircon_train\")\n",
        "\n",
        "train_cat = DatasetCatalog.get(\"zircon_train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize train dataset"
      ],
      "metadata": {
        "id": "uiWYwSS0eHWQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkNbUzUOLYf0"
      },
      "source": [
        "# visualize random sample from training dataset\n",
        "dataset_dicts = get_zircon_dicts(os.path.join(dataset_dir, 'train'))\n",
        "for d in random.sample(dataset_dicts, 4): #change int here to change sample size\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=zircon_metadata, scale=0.5)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1qJXo5foQE6"
      },
      "source": [
        "# Define save to Drive function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGQiE0Z_od7z"
      },
      "source": [
        "# a function to save models (with iteration number in name), metrics to drive; \\\n",
        "# important in case training crashes or is left unattended and disconnects. \\\n",
        "def save_outputs_to_drive(model_name, iters):\n",
        "  root_output_dir = os.path.join(model_save_dir, model_name) #output_dir = save dir from user input\n",
        "\n",
        "  #creates individual model output directory if it does not already exist\n",
        "  os.makedirs(root_output_dir, exist_ok=True)\n",
        "  #creates a name for this version of model; include iteration number\n",
        "  curr_iters_str = str(round(iters/1000, 1)) + 'k'\n",
        "  curr_model_name = model_name + '_' + curr_iters_str + '.pth'\n",
        "  model_save_pth = os.path.join(root_output_dir, curr_model_name)\n",
        "\n",
        "  #get most recent model, current metrics, copy to drive\n",
        "  model_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "  metrics_path = os.path.join(cfg.OUTPUT_DIR, 'metrics.json')\n",
        "  shutil.copy(model_path, model_save_pth)\n",
        "  shutil.copy(metrics_path, root_output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlqXIXXhW8dA"
      },
      "source": [
        "## Build, train model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set some parameters for training"
      ],
      "metadata": {
        "id": "DqUwuQxhn7aI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Add a base name for the model\n",
        "model_save_name = 'your model name here' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Final iteration before training stops\n",
        "final_iteration = 8000 #@param {type:\"slider\", min:3000, max:15000, step:1000}"
      ],
      "metadata": {
        "id": "RXvCcpkfi4yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actually build and train model"
      ],
      "metadata": {
        "id": "9VCdB008qoJv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb2K56gpnS7m"
      },
      "source": [
        "#train from a pre-trained Mask RCNN model\n",
        "cfg = get_cfg()\n",
        "\n",
        "# train from base model: Default Mask RCNN\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))\n",
        "# Load starting weights (COCO trained) from Detectron2 model zoo.\n",
        "cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x/138205316/model_final_a3ec72.pkl\"\n",
        "\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"zircon_train\",) #load training dataset\n",
        "cfg.DATASETS.TEST = (\"zircon_val\",) # load validation dataset\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2 #2 ims per batch seems to be good for model generalization\n",
        "cfg.SOLVER.BASE_LR = 0.00025  # low but reasonable learning rate given pre-training; \\\n",
        "                              # by default initializes with a 1000 iteration warmup\n",
        "cfg.SOLVER.MAX_ITER = 2000 #train for 2000 iterations before 1st save\n",
        "cfg.SOLVER.GAMMA =  0.5\n",
        "\n",
        "#decay learning rate by factor of GAMMA every 1000 iterations after 2000 iterations \\\n",
        "# and until 10000 iterations This works well for current version of training \\\n",
        "# dataset but should be modified (probably a longer interval) if dataset is ever\\\n",
        "# extended.\n",
        "cfg.SOLVER.STEPS = (1999, 2999, 3999, 4999, 5999, 6999, 7999, 8999, 9999)\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # use default ROI heads batch size\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only class here is zircon\n",
        "\n",
        "cfg.MODEL.RPN.NMS_THRESH = 0.1 #sets NMS threshold lower than default; should(?) eliminate overlapping regions\n",
        "cfg.TEST.EVAL_PERIOD = 200 # validation eval every 200 iterations\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = ZirconTrainer(cfg) #our zircon trainer, w/ built-in augs and val loss eval\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train() #start training\n",
        "\n",
        "# stop training and save for the 1st time after 2000 iterations\n",
        "save_outputs_to_drive(model_save_name, 2000)\n",
        "\n",
        "# Saves, cold restarts training from saved model weights every 1000 iterations \\\n",
        "# until final iteration. This should probably be done via hooks without stopping \\\n",
        "# training but *seems* to produce faster decrease in validation loss.\n",
        "for each_iters in [iter*1000 for iter in list(range(3, \n",
        "                                                    int(final_iteration/1000) + 1,\n",
        "                                                    1))]:\n",
        "  #reload model with last iteration model weights\n",
        "  resume_model_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "  cfg.MODEL.WEIGHTS = resume_model_path\n",
        "  cfg.SOLVER.MAX_ITER = each_iters #increase max iterations\n",
        "  trainer = ZirconTrainer(cfg)\n",
        "  trainer.resume_or_load(resume=True)\n",
        "  trainer.train() #restart training\n",
        "  #save again\n",
        "  save_outputs_to_drive(model_save_name, each_iters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBXeH8UXFcqU"
      },
      "source": [
        "# open tensorboard training metrics curves (metrics.json):\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e4vdDIOXyxF"
      },
      "source": [
        "## Inference & evaluation with final trained model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize model from saved weights:"
      ],
      "metadata": {
        "id": "J3CSgYyhtMEV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya5nEuMELeq8"
      },
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # final model; modify path to other non-final model to view their segmentations\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set a custom testing threshold\n",
        "cfg.MODEL.RPN.NMS_THRESH = 0.1\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWq1XHfDWiXO"
      },
      "source": [
        "View model segmentations for random sample of images from zircon validation dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5LhISJqWXgM"
      },
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "dataset_dicts = get_zircon_dicts(os.path.join(dataset_dir, 'val'))\n",
        "for d in random.sample(dataset_dicts, 5):    \n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=zircon_metadata, \n",
        "                   scale=1.5, \n",
        "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
        "    )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kblA1IyFvWbT"
      },
      "source": [
        "Validation eval with COCO API metric:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9tECBQCvMv3"
      },
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"zircon_val\", (\"bbox\", \"segm\"), False, output_dir=\"./output/\")\n",
        "val_loader = build_detection_test_loader(cfg, \"zircon_val\")\n",
        "print(inference_on_dataset(trainer.model, val_loader, evaluator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final notes:\n",
        "\n",
        "To use newly-trained models in colab_zirc_dims:\n",
        "\n",
        "#### Option A:\n",
        "Modify the cell that initializes model(s) in colab_zirc_dims processing notebooks:\n",
        "```\n",
        "cfg.merge_from_file(model_zoo.get_config_file(DETECTRON2 BASE CONFIG FILE LINK FOR YOUR MODEL HERE))\n",
        "cfg.MODEL.RESNETS.DEPTH = RESNET DEPTH FOR YOUR MODEL (E.G., 101) HERE\n",
        "cfg.MODEL.WEIGHTS = PATH TO YOUR MODEL IN YOUR GOOGLE DRIVE HERE\n",
        "```\n",
        "\n",
        "#### Option B (more complicated but potentially useful for many models):\n",
        "The dynamic model selection tool in colab_zirc_dims is populated from a .json file model library dictionary, which is by default [the current version on the GitHub repo.](https://github.com/MCSitar/colab_zirc_dims/blob/main/czd_model_library.json) The 'url' key in the dict will work with either an AWS download link for the model or the path to model in your Google Drive.\n",
        "\n",
        "To use a custom model library dictionary:\n",
        "Modify a copy of the colab_zirc_dims [.json file model library dictionary](https://github.com/MCSitar/colab_zirc_dims/blob/main/czd_model_library.json) to include download link(s)/Drive path(s) and metadata (e.g., resnet depth and config file) for your model(s). Upload this .json file to your Google Drive and change the 'model_lib_loc' variable in a processing Notebook to the .json's path for dynamic download and loading of this and other models within the Notebook."
      ],
      "metadata": {
        "id": "4ekgkBNOuCYs"
      }
    }
  ]
}