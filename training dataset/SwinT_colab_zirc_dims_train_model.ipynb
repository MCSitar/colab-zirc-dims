{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SwinT colab_zirc_dims_train_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnVupBBn9eR"
      },
      "source": [
        "# Zircon model training notebook; (extensively) modified from Detectron2 training tutorial\n",
        "\n",
        "This Colab Notebook will allow users to train new models to detect and segment detrital zircon from RL images using Detectron2 and the training dataset provided in the colab_zirc_dims repo. It is set up to train a Mask RCNN model with a Swin transformer backbone ([unofficial Detectron2 implementation](https://github.com/xiaohu2015/SwinT_detectron2)).\n",
        "\n",
        "The training dataset should be uploaded to the user's Google Drive before running this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII"
      },
      "source": [
        "## Install detectron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsePPpwZSmqt"
      },
      "source": [
        "!pip install pyyaml==5.1\n",
        "\n",
        "import torch\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "# Install detectron2 that matches the above pytorch version\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n",
        "!pip install timm\n",
        "!git clone https://github.com/xiaohu2015/SwinT_detectron2 swinT_repo\n",
        "exit(0)  # Automatically restarts runtime after installation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/swinT_repo')"
      ],
      "metadata": {
        "id": "z5KN4Y6bjKys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF"
      },
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "import copy\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "import random\n",
        "import shutil\n",
        "import torch\n",
        "\n",
        "import swint\n",
        "# import some common detectron2 utilities\n",
        "from detectron2.engine.hooks import HookBase\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.evaluation import inference_context #COCOEvaluator\n",
        "from detectron2.evaluation import COCOEvaluator\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.utils.logger import log_every_n_seconds\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_train_loader, DatasetMapper, build_detection_test_loader\n",
        "import detectron2.utils.comm as comm\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2.config import LazyConfig\n",
        "import detectron2.data.transforms as T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1-3zSWRn4H3"
      },
      "source": [
        "## Define Augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below defines augmentations used while training to ensure that models never see the same exact image twice during training. This mitigates overfitting and allows models to achieve substantially higher accuracy in their segmentations/measurements."
      ],
      "metadata": {
        "id": "VxelWqWDY7nZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VU2nQL3n63D"
      },
      "source": [
        "custom_transform_list = [\n",
        "                         T.RandomCrop('relative', (0.95, 0.95)), #randomly crop an area (95% size of original) from image\n",
        "                         T.RandomLighting(100), #minor lighting randomization\n",
        "                         T.RandomContrast(.85, 1.15), #minor contrast randomization\n",
        "                         T.RandomFlip(prob=.5, horizontal=False, vertical=True), #random vertical flipping\n",
        "                         T.RandomFlip(prob=.5, horizontal=True, vertical=False),  #and horizontal flipping\n",
        "                         T.RandomApply(T.RandomRotation([-30, 30], False), prob=.8), #random (80% probability) rotation up to 30 degrees; \\\n",
        "                                                                                     # more rotation does not seem to improve results\n",
        "                         T.ResizeShortestEdge([400, 800])] # random resize short edge (multi-scale/resolution training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive, set paths to dataset, model saving directories"
      ],
      "metadata": {
        "id": "b1mFydlUY00A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fBfk6JlPYypJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Add path to training dataset directory\n",
        "dataset_dir = '/content/drive/MyDrive/TRAINING_DATASET_HERE' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Add path to model saving directory (automatically created if it does not yet exist)\n",
        "model_save_dir = '/content/drive/MyDrive/SAVE_DIR_FOR_NEW_MODELS_HERE' #@param {type:\"string\"}\n",
        "\n",
        "os.makedirs(model_save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "90RmxiInbQLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2bjrfb2LDeo"
      },
      "source": [
        "## Define dataset mapper, training, loss eval functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIbAM2pv-urF"
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.data import DatasetMapper\n",
        "from detectron2.structures import BoxMode\n",
        "\n",
        "# a function to convert Via image annotation .json dict format to Detectron2 \\\n",
        "# training input dict format\n",
        "def get_zircon_dicts(img_dir):\n",
        "    json_file = os.path.join(img_dir, \"via_region_data.json\")\n",
        "    with open(json_file) as f:\n",
        "        imgs_anns = json.load(f)['_via_img_metadata']\n",
        "\n",
        "    dataset_dicts = []\n",
        "    for idx, v in enumerate(imgs_anns.values()):\n",
        "        record = {}\n",
        "        \n",
        "        filename = os.path.join(img_dir, v[\"filename\"])\n",
        "        height, width = cv2.imread(filename).shape[:2]\n",
        "        \n",
        "        record[\"file_name\"] = filename\n",
        "        record[\"image_id\"] = idx\n",
        "        record[\"height\"] = height\n",
        "        record[\"width\"] = width\n",
        "      \n",
        "        #annos = v[\"regions\"]\n",
        "        annos = {}\n",
        "        for n, eachitem in enumerate(v['regions']):\n",
        "          annos[str(n)] = eachitem\n",
        "        objs = []\n",
        "        for _, anno in annos.items():\n",
        "            #assert not anno[\"region_attributes\"]\n",
        "            anno = anno[\"shape_attributes\"]\n",
        "            px = anno[\"all_points_x\"]\n",
        "            py = anno[\"all_points_y\"]\n",
        "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
        "            poly = [p for x in poly for p in x]\n",
        "\n",
        "            obj = {\n",
        "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
        "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
        "                \"segmentation\": [poly],\n",
        "                \"category_id\": 0,\n",
        "            }\n",
        "            objs.append(obj)\n",
        "        record[\"annotations\"] = objs\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "# loss eval hook for getting vaidation loss, copying to metrics.json; \\\n",
        "# from https://gist.github.com/ortegatron/c0dad15e49c2b74de8bb09a5615d9f6b\n",
        "class LossEvalHook(HookBase):\n",
        "    def __init__(self, eval_period, model, data_loader):\n",
        "        self._model = model\n",
        "        self._period = eval_period\n",
        "        self._data_loader = data_loader\n",
        "    \n",
        "    def _do_loss_eval(self):\n",
        "        # Copying inference_on_dataset from evaluator.py\n",
        "        total = len(self._data_loader)\n",
        "        num_warmup = min(5, total - 1)\n",
        "            \n",
        "        start_time = time.perf_counter()\n",
        "        total_compute_time = 0\n",
        "        losses = []\n",
        "        for idx, inputs in enumerate(self._data_loader):            \n",
        "            if idx == num_warmup:\n",
        "                start_time = time.perf_counter()\n",
        "                total_compute_time = 0\n",
        "            start_compute_time = time.perf_counter()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            total_compute_time += time.perf_counter() - start_compute_time\n",
        "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
        "            seconds_per_img = total_compute_time / iters_after_start\n",
        "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
        "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
        "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
        "                log_every_n_seconds(\n",
        "                    logging.INFO,\n",
        "                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
        "                        idx + 1, total, seconds_per_img, str(eta)\n",
        "                    ),\n",
        "                    n=5,\n",
        "                )\n",
        "            loss_batch = self._get_loss(inputs)\n",
        "            losses.append(loss_batch)\n",
        "        mean_loss = np.mean(losses)\n",
        "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
        "        comm.synchronize()\n",
        "\n",
        "        return losses\n",
        "            \n",
        "    def _get_loss(self, data):\n",
        "        # How loss is calculated on train_loop \n",
        "        metrics_dict = self._model(data)\n",
        "        metrics_dict = {\n",
        "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
        "            for k, v in metrics_dict.items()\n",
        "        }\n",
        "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
        "        return total_losses_reduced\n",
        "        \n",
        "        \n",
        "    def after_step(self):\n",
        "        next_iter = self.trainer.iter + 1\n",
        "        is_final = next_iter == self.trainer.max_iter\n",
        "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
        "            self._do_loss_eval()\n",
        "\n",
        "#trainer for zircons which incorporates augmentation, hooks for eval\n",
        "class ZirconTrainer(DefaultTrainer):\n",
        "    \n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        #return a custom train loader with augmentations; recompute_boxes \\\n",
        "        # is important given cropping, rotation augs\n",
        "        return build_detection_train_loader(cfg, mapper=\n",
        "                                            DatasetMapper(cfg, is_train=True, recompute_boxes = True,\n",
        "                                                          augmentations = custom_transform_list\n",
        "                                                          ),\n",
        "                                            )\n",
        "\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "        if output_folder is None:\n",
        "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
        "    \n",
        "    #set up validation loss eval hook\n",
        "    def build_hooks(self):\n",
        "        hooks = super().build_hooks()\n",
        "        hooks.insert(-1,LossEvalHook(\n",
        "            cfg.TEST.EVAL_PERIOD,\n",
        "            self.model,\n",
        "            build_detection_test_loader(\n",
        "                self.cfg,\n",
        "                self.cfg.DATASETS.TEST[0],\n",
        "                DatasetMapper(self.cfg,True)\n",
        "            )\n",
        "        ))\n",
        "        return hooks\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bksg-AInoIeZ"
      },
      "source": [
        "## Import train, val catalogs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9y0C9pUbOHN"
      },
      "source": [
        "#registers training, val datasets (converts annotations using get_zircon_dicts)\n",
        "for d in [\"train\", \"val\"]:\n",
        "    DatasetCatalog.register(\"zircon_\" + d, lambda d=d: get_zircon_dicts(dataset_dir + \"/\" + d))\n",
        "    MetadataCatalog.get(\"zircon_\" + d).set(thing_classes=[\"zircon\"])\n",
        "zircon_metadata = MetadataCatalog.get(\"zircon_train\")\n",
        "\n",
        "train_cat = DatasetCatalog.get(\"zircon_train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize train dataset"
      ],
      "metadata": {
        "id": "uiWYwSS0eHWQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkNbUzUOLYf0"
      },
      "source": [
        "# visualize random sample from training dataset\n",
        "dataset_dicts = get_zircon_dicts(os.path.join(dataset_dir, 'train'))\n",
        "for d in random.sample(dataset_dicts, 4): #change int here to change sample size\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=zircon_metadata, scale=0.5)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1qJXo5foQE6"
      },
      "source": [
        "# Define save to Drive function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGQiE0Z_od7z"
      },
      "source": [
        "# a function to save models (with iteration number in name), metrics to drive; \\\n",
        "# important in case training crashes or is left unattended and disconnects. \\\n",
        "def save_outputs_to_drive(model_name, iters):\n",
        "  root_output_dir = os.path.join(model_save_dir, model_name) #output_dir = save dir from user input\n",
        "\n",
        "  #creates individual model output directory if it does not already exist\n",
        "  os.makedirs(root_output_dir, exist_ok=True)\n",
        "  #creates a name for this version of model; include iteration number\n",
        "  curr_iters_str = str(round(iters/1000, 1)) + 'k'\n",
        "  curr_model_name = model_name + '_' + curr_iters_str + '.pth'\n",
        "  model_save_pth = os.path.join(root_output_dir, curr_model_name)\n",
        "\n",
        "  #get most recent model, current metrics, copy to drive\n",
        "  model_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "  metrics_path = os.path.join(cfg.OUTPUT_DIR, 'metrics.json')\n",
        "  shutil.copy(model_path, model_save_pth)\n",
        "  shutil.copy(metrics_path, root_output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlqXIXXhW8dA"
      },
      "source": [
        "## Build, train model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set some parameters for training"
      ],
      "metadata": {
        "id": "DqUwuQxhn7aI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Add a base name for the model\n",
        "model_save_name = 'MODEL SAVE NAME HERE' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Final iteration before training stops\n",
        "final_iteration = 12000 #@param {type:\"slider\", min:3000, max:15000, step:1000}"
      ],
      "metadata": {
        "id": "RXvCcpkfi4yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actually build and train model"
      ],
      "metadata": {
        "id": "9VCdB008qoJv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb2K56gpnS7m"
      },
      "source": [
        "#train from a pre-trained Mask RCNN model\n",
        "cfg = get_cfg()\n",
        "swint.add_swint_config(cfg)\n",
        "\n",
        "# train from base model: Default Mask RCNN\n",
        "cfg.merge_from_file('/content/swinT_repo/configs/SwinT/mask_rcnn_swint_T_FPN_3x.yaml')\n",
        "# Load starting weights (COCO trained) from Detectron2 model zoo.\n",
        "cfg.MODEL.WEIGHTS = \"https://github.com/xiaohu2015/SwinT_detectron2/releases/download/v1.0/mask_rcnn_swint_T_coco17.pth\"\n",
        "\n",
        "cfg.OUTPUT_DIR = '/content/outputs'\n",
        "cfg.DATASETS.TRAIN = (\"zircon_train\",) #load training dataset\n",
        "cfg.DATASETS.TEST = (\"zircon_val\",) # load validation dataset\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2 #2 ims per batch seems to be good for model generalization\n",
        "cfg.SOLVER.BASE_LR = 0.0005  # starting LR 2x that of Resnet Mask RCNN models; \\\n",
        "                              # by default initializes with a 1000 iteration warmup\n",
        "\n",
        "\n",
        "cfg.SOLVER.MAX_ITER = 1500 #train for 1500 iterations before 1st save\n",
        "cfg.SOLVER.GAMMA =  0.5\n",
        "cfg.MODEL.PIXEL_MEAN = [118.45, 119.54, 132.12]\n",
        "\n",
        "\n",
        "#decay learning rate by factor of GAMMA every 1000 iterations after 1499 iterations \\\n",
        "# and until 10000 iterations This works well for current version of training \\\n",
        "# dataset but should be modified (probably a longer interval) if dataset is ever\\\n",
        "# extended.\n",
        "cfg.SOLVER.STEPS = (1499, 2999, 3999, 4999, 5999, 6999, 7999, 8999, 9999)\n",
        "#old cfg.SOLVER.STEPS = (1499, 2999, 3999, 4999, 5999, 6999, 7999, 8999, 9999)\n",
        "\n",
        "#cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # use default ROI heads batch size\n",
        "#cfg.MODEL.FCOS.NUM_CLASSES = 1  # only class here is zircon\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
        "\n",
        "cfg.MODEL.RPN.NMS_THRESH = 0.4\n",
        "#cfg.MODEL.FCOS.NMS_TH = 0.4 #sets NMS threshold lower than default; should(?) eliminate overlapping regions\n",
        "cfg.TEST.EVAL_PERIOD = 200 # validation eval every 200 iterations\n",
        "\n",
        "#Weight decay used during training for SwinT publication, so included here; \\\n",
        "# default value (from detectron2-swint) for COCO is .05, which works well with \\\n",
        "# starting LR of 0.0005. Weight decay is adjusted with decreases in learning rate \\\n",
        "# (see below).\n",
        "curr_weight_decay = cfg.SOLVER.WEIGHT_DECAY\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "trainer = ZirconTrainer(cfg) #our zircon trainer, w/ built-in augs and val loss eval\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train() #start training\n",
        "\n",
        "# stop training and save for the 1st time after 1500 iterations\n",
        "save_outputs_to_drive(model_save_name, 1500)\n",
        "\n",
        "# Saves, cold restarts training from saved model weights every 1000 iterations \\\n",
        "# until final iteration. This should probably be done via hooks without stopping \\\n",
        "# training but *seems* to produce faster decrease in validation loss.\n",
        "for each_iters in [iter*1000 for iter in list(range(2, \n",
        "                                                    int(final_iteration/1000) + 1,\n",
        "                                                    1))]:\n",
        "  #reload model with last iteration model weights\n",
        "  resume_model_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "  cfg.MODEL.WEIGHTS = resume_model_path\n",
        "\n",
        "  #decrease weight decay twice as fast as decrease learning rate \\\n",
        "  # (prevents decay from outstripping learning)\n",
        "  curr_weight_decay = (cfg.SOLVER.GAMMA/2) * cfg.SOLVER.WEIGHT_DECAY\n",
        "  cfg.SOLVER.WEIGHT_DECAY = curr_weight_decay\n",
        "\n",
        "  cfg.SOLVER.MAX_ITER = each_iters #increase max iterations\n",
        "  trainer = ZirconTrainer(cfg)\n",
        "  trainer.resume_or_load(resume=True)\n",
        "  trainer.train() #restart training\n",
        "  #save again\n",
        "  save_outputs_to_drive(model_save_name, each_iters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBXeH8UXFcqU"
      },
      "source": [
        "# open tensorboard training metrics curves (metrics.json):\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e4vdDIOXyxF"
      },
      "source": [
        "## Inference & evaluation with final trained model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize model from saved weights:"
      ],
      "metadata": {
        "id": "J3CSgYyhtMEV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya5nEuMELeq8"
      },
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # final model; modify path to other non-final model to view their segmentations\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8  # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWq1XHfDWiXO"
      },
      "source": [
        "View model segmentations for random sample of images from zircon validation dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5LhISJqWXgM"
      },
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "dataset_dicts = get_zircon_dicts(os.path.join(dataset_dir, 'val'))\n",
        "for d in random.sample(dataset_dicts, 31):\n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=zircon_metadata, \n",
        "                   scale=1.5 #, \n",
        "    )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kblA1IyFvWbT"
      },
      "source": [
        "Validation eval with COCO API metric:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9tECBQCvMv3"
      },
      "source": [
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"zircon_val\", (\"bbox\", \"segm\"), False, output_dir=\"./output/\")\n",
        "val_loader = build_detection_test_loader(cfg, \"zircon_val\")\n",
        "print(inference_on_dataset(trainer.model, val_loader, evaluator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final notes:\n",
        "\n",
        "To use newly-trained models in colab_zirc_dims:\n",
        "\n",
        "See .txt file in data repo."
      ],
      "metadata": {
        "id": "4ekgkBNOuCYs"
      }
    }
  ]
}